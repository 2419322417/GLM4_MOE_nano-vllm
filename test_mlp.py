import torch
import torch.nn as nn
import os
from safetensors.torch import load_file
from transformers import AutoConfig
from nanovllm.models.glm4_moe.mlp import Glm4MoeMLP
import torch.testing as tt

# def main():
#     """
#     ä½¿ç”¨ä» FH_DEBUG å¯¼å‡ºçš„å¼ é‡æ–‡ä»¶æµ‹è¯• Glm4MoeMLPã€‚
#     éªŒè¯ forward è¾“å‡ºæ˜¯å¦ä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚
#     """

#     torch.set_default_device("cuda")
#     torch.set_default_dtype(torch.float16)

#     # -------------------------------------------------------------------------
#     # 1. åŠ è½½æ¨¡å‹é…ç½®
#     # -------------------------------------------------------------------------
#     model = "/data/model/ZhipuAI/GLM-4.5-Air"
#     config = AutoConfig.from_pretrained(model)
#     hidden_size = config.hidden_size
#     intermediate_size = config.intermediate_size
#     print(f"âœ… hidden_size = {hidden_size}, intermediate_size = {intermediate_size}")

#     # -------------------------------------------------------------------------
#     # 2. æŒ‡å®š safetensors æ•°æ®æ–‡ä»¶è·¯å¾„
#     # -------------------------------------------------------------------------
#     tensor_path = "/data/ai_infra/debug/tensors1/rank_0/model.layers.0.mlp_2.safetensors"
#     assert os.path.exists(tensor_path), f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {tensor_path}"
#     print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å¼ é‡æ–‡ä»¶: {tensor_path}")

#     from safetensors import safe_open
#     with safe_open(tensor_path, framework="pt") as f:
#         x = f.get_tensor("x").to("cuda")
#         ref_output = f.get_tensor("ret_x").to("cuda")

#     print(f"ğŸ“¦ è¾“å…¥å¼ é‡ hidden_states å½¢çŠ¶: {x.shape}")
#     print(f"ğŸ“¦ å‚è€ƒè¾“å‡ºå¼ é‡ output å½¢çŠ¶: {ref_output.shape}")

#     # -------------------------------------------------------------------------
#     # 3. åˆå§‹åŒ–è¦æµ‹è¯•çš„ MLP å±‚
#     # -------------------------------------------------------------------------
#     # prefix = "model.layers.0.mlp"
#     prefix = "model.layers.0.mlp_1.safetensors"
#     mlp = Glm4MoeMLP(
#         hidden_size=hidden_size,
#         intermediate_size=intermediate_size,
#         hidden_act="silu",
#         prefix=prefix,
#     ).to("cuda").half()

#     # âœ… åŠ è½½å¯¹åº” safetensors æƒé‡
#     mlp.load_from_model(model, prefix)

#     # -------------------------------------------------------------------------
#     # 4. å‰å‘æ¨ç†
#     # -------------------------------------------------------------------------
#     print("ğŸš€ å¼€å§‹æ‰§è¡Œå‰å‘æ¨ç†...")
#     output = mlp(x)
#     print(f"âœ… æ¨ç†è¾“å‡ºå¼ é‡å½¢çŠ¶: {output.shape}")

#     # -------------------------------------------------------------------------
#     # 5. å°ºå¯¸éªŒè¯
#     # -------------------------------------------------------------------------
#     assert output.shape == ref_output.shape, (
#         f"âŒ è¾“å‡ºå°ºå¯¸ä¸åŒ¹é…! æ¨¡å‹è¾“å‡º: {output.shape}, å‚è€ƒè¾“å‡º: {ref_output.shape}"
#     )

#     # -------------------------------------------------------------------------
#     # 6. è®¡ç®—è¯¯å·®
#     # -------------------------------------------------------------------------
#     diff = torch.abs(output - ref_output)
#     max_diff = diff.max().item()
#     mean_diff = diff.mean().item()
#     print(f"ğŸ“Š æœ€å¤§ç»å¯¹è¯¯å·®: {max_diff:.6f}, å¹³å‡è¯¯å·®: {mean_diff:.6f}")

#     # è®¾ç½®å®¹å¿é˜ˆå€¼ï¼ˆFP16 ç²¾åº¦ï¼‰
#     if max_diff < 1e-2:
#         print("ğŸ¯ éªŒè¯é€šè¿‡ï¼šè¾“å‡ºä¸å¯¼å‡ºå¼ é‡é«˜åº¦ä¸€è‡´ âœ…")
#     else:
#         print("âš ï¸ æ³¨æ„ï¼šè¾“å‡ºä¸ä¿å­˜çš„å¼ é‡å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å‚æ•°æˆ–é‡åŒ–é…ç½®ã€‚")


# if __name__ == "__main__":
#     main()








#---------------------é€å±‚debugä»£ç ç‰‡æ®µ---------------------#
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import torch
from safetensors.torch import load_file

from nanovllm.models.glm4_moe.mlp import Glm4MoeMLP

MODEL_DIR = "/data/model/ZhipuAI/GLM-4.5-Air"
TENSOR_PATH = "/data/ai_infra/debug/tensors3/rank_0/model.layers.0.mlp_4.safetensors"

HIDDEN_SIZE = 4096
INTERMEDIATE_SIZE = 10944
PREFIX = "model.layers.0.mlp"

ERROR_THRESHOLD = 1e-2   # ä½ å¯ä»¥æ”¹æˆ 1e-4 / 3e-3 ç­‰


def check_and_report(name, pred, ref, bad_layers):
    if pred.shape != ref.shape:
        print(f"âŒ {name}: shape mismatch! pred={pred.shape}, ref={ref.shape}")
        bad_layers.append(name)
        return

    diff = (pred - ref).abs()
    max_err = diff.max().item()
    mean_err = diff.mean().item()
    print(f"{name:>8}: max_err={max_err:.6f}, mean_err={mean_err:.6f}")

    if max_err > ERROR_THRESHOLD:
        bad_layers.append(name)


def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16  

    print(f"ğŸ“‚ åŠ è½½å¼ é‡æ–‡ä»¶: {TENSOR_PATH}")
    tensors = load_file(TENSOR_PATH)

    # æ£€æŸ¥ keys
    needed = ["x", "gate_up", "gate", "up", "act_x", "down", "ret_x"]
    for k in needed:
        if k not in tensors:
            print(f"âš ï¸ ç¼ºå°‘å¼ é‡: {k}")

    x_ref = tensors["x"].to(device=device, dtype=dtype)
    print(f"ğŸ“¦ è¾“å…¥ x å½¢çŠ¶: {x_ref.shape}")

    # åŠ è½½ nano MLP
    mlp = Glm4MoeMLP(
        hidden_size=HIDDEN_SIZE,
        intermediate_size=INTERMEDIATE_SIZE,
        hidden_act="silu",
        quant_config=None,
        prefix=PREFIX,
    )
    mlp.load_from_model(MODEL_DIR, PREFIX)
    mlp.to(device=device, dtype=dtype)
    mlp.eval()

    bad_layers = []

    with torch.no_grad():
        print("gate_up_proj.weight shape:", mlp.gate_up_proj.weight.shape)
        W = mlp.gate_up_proj.weight.detach().cpu()

        # åˆ‡ä¸¤ä¸ªåˆ†æ”¯
        gate_W = W[:10944,:]
        up_W   = W[10944:,:]

        print("gate abs mean:", gate_W.abs().mean().item())
        print("up   abs mean:", up_W.abs().mean().item())






        # x
        # check_and_report("x", x_ref, x_ref, bad_layers)
        torch.testing.assert_close(x_ref, x_ref, rtol=1e-03, atol=1e-03)

        # gate_up
        gate_up_pred = mlp.gate_up_proj(x_ref)
        # check_and_report("gate_up", gate_up_pred, tensors["gate_up"].to(device, dtype), bad_layers)
        torch.testing.assert_close(gate_up_pred, tensors["gate_up"].to(device, dtype), rtol=1e-03, atol=1e-03)

        # gate / up
        gate_pred, up_pred = gate_up_pred.chunk(2, dim=-1)
        # print(f"gate_pred shape: {gate_pred.shape},gate_ref shape: {tensors['gate'].shape}")
        # print(f"up_pred shape: {up_pred.shape},up_ref shape: {tensors['up'].shape}")
        # check_and_report("gate", gate_pred, tensors["gate"].to(device, dtype), bad_layers)
        # check_and_report("up", up_pred, tensors["up"].to(device, dtype), bad_layers)
        torch.testing.assert_close(gate_pred, tensors["gate"].to(device, dtype), rtol=1e-03, atol=1e-03)
        torch.testing.assert_close(up_pred, tensors["up"].to(device, dtype), rtol=1e-03, atol=1e-03)

        # Silu(gate) * up
        gate_act_pred = torch.nn.functional.silu(gate_pred)
        act_x_pred = gate_act_pred * up_pred
        # check_and_report("act_x", act_x_pred, tensors["act_x"].to(device, dtype), bad_layers)
        torch.testing.assert_close(act_x_pred, tensors["act_x"].to(device, dtype), rtol=1e-03, atol=1e-03)

        # down_proj
        down_pred = mlp.down_proj(act_x_pred)
        # check_and_report("down", down_pred, tensors["down"].to(device, dtype), bad_layers)
        torch.testing.assert_close(down_pred, tensors["ret_x"].to(device, dtype), rtol=1e-03, atol=1e-03)

        # ret_x
        ret_x_pred = down_pred
        # check_and_report("ret_x", ret_x_pred, tensors["ret_x"].to(device, dtype), bad_layers)
        torch.testing.assert_close(ret_x_pred, tensors["ret_x"].to(device, dtype), rtol=1e-03, atol=1e-03)

    print("\n==============================")
    if len(bad_layers) == 0:
        print("ğŸ‰ æ‰€æœ‰å±‚ **å®Œå…¨å¯¹é½**ï¼Œæ— è¯¯å·®è¿‡å¤§å±‚ï¼")
    else:
        print("âŒ ä»¥ä¸‹å±‚è¯¯å·®è¶…è¿‡é˜ˆå€¼:")
        for name in bad_layers:
            print(f"   - {name}")
        print("==============================")
        print("âš ï¸ æ ¹æ®ç¬¬ä¸€æ¬¡å‡ºé”™çš„å±‚ï¼Œå»æ£€æŸ¥å¯¹åº”ç®—å­ / æƒé‡åŠ è½½ã€‚")
    print("==============================\n")


if __name__ == "__main__":
    main()
# ------------------- Glm4MoeMLP é€å±‚debugä»£ç ç‰‡æ®µ ------------------- #