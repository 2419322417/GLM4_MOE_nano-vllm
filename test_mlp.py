import torch
import torch.nn as nn
import os
from safetensors.torch import load_file
from transformers import AutoConfig
from nanovllm.models.glm4_moe.mlp import Glm4MoeMLP


def main():
    """
    ä½¿ç”¨ä» FH_DEBUG å¯¼å‡ºçš„å¼ é‡æ–‡ä»¶æµ‹è¯• Glm4MoeMLPã€‚
    éªŒè¯ forward è¾“å‡ºæ˜¯å¦ä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚
    """

    torch.set_default_device("cuda")
    torch.set_default_dtype(torch.float16)

    # -------------------------------------------------------------------------
    # 1. åŠ è½½æ¨¡å‹é…ç½®
    # -------------------------------------------------------------------------
    model = "/data/model/ZhipuAI/GLM-4.5-Air"
    config = AutoConfig.from_pretrained(model)
    hidden_size = config.hidden_size
    intermediate_size = config.intermediate_size
    print(f"âœ… hidden_size = {hidden_size}, intermediate_size = {intermediate_size}")

    # -------------------------------------------------------------------------
    # 2. æŒ‡å®š safetensors æ•°æ®æ–‡ä»¶è·¯å¾„
    # -------------------------------------------------------------------------
    tensor_path = "/data/ai_infra/debug/tensors1/rank_0/model.layers.0.mlp_2.safetensors"
    assert os.path.exists(tensor_path), f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {tensor_path}"
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å¼ é‡æ–‡ä»¶: {tensor_path}")

    from safetensors import safe_open
    with safe_open(tensor_path, framework="pt") as f:
        x = f.get_tensor("x").to("cuda")
        ref_output = f.get_tensor("ret_x").to("cuda")

    print(f"ğŸ“¦ è¾“å…¥å¼ é‡ hidden_states å½¢çŠ¶: {x.shape}")
    print(f"ğŸ“¦ å‚è€ƒè¾“å‡ºå¼ é‡ output å½¢çŠ¶: {ref_output.shape}")

    # -------------------------------------------------------------------------
    # 3. åˆå§‹åŒ–è¦æµ‹è¯•çš„ MLP å±‚
    # -------------------------------------------------------------------------
    # prefix = "model.layers.0.mlp"
    prefix = "model.layers.0.mlp_1.safetensors"
    mlp = Glm4MoeMLP(
        hidden_size=hidden_size,
        intermediate_size=intermediate_size,
        hidden_act="silu",
        prefix=prefix,
    ).to("cuda").half()

    # âœ… åŠ è½½å¯¹åº” safetensors æƒé‡
    mlp.load_from_model(model, prefix)

    # -------------------------------------------------------------------------
    # 4. å‰å‘æ¨ç†
    # -------------------------------------------------------------------------
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå‰å‘æ¨ç†...")
    output = mlp(x)
    print(f"âœ… æ¨ç†è¾“å‡ºå¼ é‡å½¢çŠ¶: {output.shape}")

    # -------------------------------------------------------------------------
    # 5. å°ºå¯¸éªŒè¯
    # -------------------------------------------------------------------------
    assert output.shape == ref_output.shape, (
        f"âŒ è¾“å‡ºå°ºå¯¸ä¸åŒ¹é…! æ¨¡å‹è¾“å‡º: {output.shape}, å‚è€ƒè¾“å‡º: {ref_output.shape}"
    )

    # -------------------------------------------------------------------------
    # 6. è®¡ç®—è¯¯å·®
    # -------------------------------------------------------------------------
    diff = torch.abs(output - ref_output)
    max_diff = diff.max().item()
    mean_diff = diff.mean().item()
    print(f"ğŸ“Š æœ€å¤§ç»å¯¹è¯¯å·®: {max_diff:.6f}, å¹³å‡è¯¯å·®: {mean_diff:.6f}")

    # è®¾ç½®å®¹å¿é˜ˆå€¼ï¼ˆFP16 ç²¾åº¦ï¼‰
    if max_diff < 1e-2:
        print("ğŸ¯ éªŒè¯é€šè¿‡ï¼šè¾“å‡ºä¸å¯¼å‡ºå¼ é‡é«˜åº¦ä¸€è‡´ âœ…")
    else:
        print("âš ï¸ æ³¨æ„ï¼šè¾“å‡ºä¸ä¿å­˜çš„å¼ é‡å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å‚æ•°æˆ–é‡åŒ–é…ç½®ã€‚")


if __name__ == "__main__":
    main()
