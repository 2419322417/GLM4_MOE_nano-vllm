import torch
import torch.nn as nn
import os
from nanovllm.layers.activation import SiluAndMul
from nanovllm.layers.linear import MergedColumnParallelLinear, RowParallelLinear
from typing import Optional
from safetensors import safe_open
from safetensors.torch import load_file



class Glm4MoeMLP(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        # nanoç‰ˆ æ›¿æ¢QuantizationConfigå¯¹vllmçš„ä¾èµ–ã€‚
        quant_config: Optional[object] = None,
        reduce_results: bool = True,
        prefix: str = "",
    ) -> None:
        super().__init__()

        #nano æ›¿æ¢MergedColumnParallelLinearä¾èµ–
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,  # âœ… å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œè¡¨ç¤ºä¸¤ä¸ªåˆ†æ”¯
            bias=False
        )

        # nanoç‰ˆ æ›¿æ¢RowParallelLinearä¾èµ–
        self.down_proj = RowParallelLinear(
            intermediate_size, 
            hidden_size, 
            bias=False
        )

        # æ¿€æ´»å‡½æ•°æ£€æŸ¥ï¼Œä¸ºäº†ä»£ç å¥å£®æ€§ï¼Œå®åˆ™æ²¡å•¥ç”¨ï¼Œå°±æ˜¯ä¸ºäº†é˜²æ­¢ä¼ å…¥å¥‡æ€ªçš„æ¿€æ´»å‡½æ•°ã€‚
        if hidden_act.lower() != "silu":
            raise ValueError(
                f"Unsupported activation: {hidden_act}. Only silu is supported for now."
            )
        self.act_fn = SiluAndMul()
        self.prefix = prefix
        self.debug_id = 0

    def forward(self, x):
        gate_up  = self.gate_up_proj(x) 
        act_x = self.act_fn(gate_up)
        ret_x = self.down_proj(act_x)

        return ret_x
    

    def load_from_model(self, model_dir: str, prefix: str):
        """ä» vLLM å¯¼å‡ºçš„ safetensors æƒé‡ä¸­åŠ è½½å½“å‰ MLP å±‚å‚æ•°"""
        import glob
        weight_files = sorted(glob.glob(f"{model_dir}/*.safetensors"))
        print(f"ğŸ“¦ æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡åˆ†ç‰‡")

        # éå†æ‰€æœ‰æƒé‡æ–‡ä»¶ï¼Œæ‰¾åˆ°å¯¹åº”å±‚çš„å‚æ•°
        for wf in weight_files:
            tensors = load_file(wf)
            for name, tensor in tensors.items():
                if name.startswith(prefix):
                    if "gate_up_proj" in name:
                        print(f"âœ… åŠ è½½ {name}")
                        self.gate_up_proj.weight.data.copy_(tensor)
                    elif "down_proj" in name:
                        print(f"âœ… åŠ è½½ {name}")
                        self.down_proj.weight.data.copy_(tensor)
    
def main():
    """
    ä½¿ç”¨ä» vLLM å¯¼å‡ºçš„ safetensors æ–‡ä»¶éªŒè¯ Glm4MoeMLP å±‚ä¸€è‡´æ€§
    """
    from transformers import AutoConfig, Glm4MoeConfig

    torch.manual_seed(42)
    torch.set_default_device("cuda")
    torch.set_default_dtype(torch.float16)

    # -------------------------------------------------------------------------
    # 1. åŠ è½½æ¨¡å‹é…ç½®
    # -------------------------------------------------------------------------
    model = "/data/model/ZhipuAI/GLM-4.5-Air"
    config = AutoConfig.from_pretrained(model)
    config = Glm4MoeConfig(config)

    # -------------------------------------------------------------------------
    # 2. åˆå§‹åŒ–è¦æµ‹è¯•çš„ MLP å±‚
    # -------------------------------------------------------------------------
    prefix = "model.layers.1.mlp"  # é€‰æ‹©å¯¹åº”å±‚ï¼ˆå¯ä»¥æ›´æ”¹ä¸ºå…¶ä»–å±‚è¿›è¡ŒéªŒè¯ï¼‰
    from transformers.models.glm4_moe.modeling_glm4_moe import Glm4MoeMLP
    mlp = Glm4MoeMLP(
        hidden_size=config.hidden_size,
        intermediate_size=config.intermediate_size,
        hidden_act = config.hidden_act,
        prefix = prefix,
    )
    mlp.load_weights(model)

    # -------------------------------------------------------------------------
    # 3. åŠ è½½ vLLM å¯¼å‡ºçš„å‚è€ƒè¾“å…¥ä¸è¾“å‡º
    # -------------------------------------------------------------------------
    import safetensors
    sample_path = "/data/ai_infra/debug/glm4-6-awq-tensors"
    tensor_path = os.path.join(sample_path, f"{prefix}_0.safetensors")

    if not os.path.exists(tensor_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {tensor_path}")

    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½è°ƒè¯•å¼ é‡: {tensor_path}")
    loaded_tensor = safetensors.torch.load_file(tensor_path)

    x = loaded_tensor["x"]  # MLPè¾“å…¥
    output_reference = loaded_tensor["ret_x"]  # vLLMè¾“å‡ºï¼ˆå‚è€ƒå€¼ï¼‰

    print(f"âœ… æˆåŠŸåŠ è½½è¾“å…¥å¼ é‡: {x.shape}")
    print(f"âœ… æˆåŠŸåŠ è½½è¾“å‡ºå¼ é‡: {output_reference.shape}")

    # -------------------------------------------------------------------------
    # 4. å‰å‘æ¨ç†å¹¶æ¯”è¾ƒç»“æœ
    # -------------------------------------------------------------------------
    print("ğŸš€ å¼€å§‹æ‰§è¡Œå‰å‘æ¨ç†...")
    output = mlp(x)

    print("ğŸ§® æ¯”è¾ƒè¾“å‡ºç»“æœ...")
    torch.testing.assert_close(output, output_reference, rtol=1e-3, atol=1e-3)
    print("âœ… MLP å±‚è¾“å‡ºä¸ vLLM ä¸€è‡´ï¼ŒéªŒè¯é€šè¿‡ï¼")

if __name__ == "__main__":
    main()