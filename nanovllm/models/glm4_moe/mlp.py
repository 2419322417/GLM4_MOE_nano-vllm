import torch
import torch.nn as nn
import os
from nanovllm.layers.activation import SiluAndMul
from nanovllm.layers.linear import MergedColumnParallelLinear, RowParallelLinear
from typing import Optional
from safetensors import safe_open
from safetensors.torch import load_file
from glob import glob


class Glm4MoeMLP(nn.Module):

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        # nanoç‰ˆ æ›¿æ¢QuantizationConfigå¯¹vllmçš„ä¾èµ–ã€‚
        quant_config: Optional[object] = None,
        reduce_results: bool = True,
        prefix: str = "",
    ) -> None:
        super().__init__()

        #nano æ›¿æ¢MergedColumnParallelLinearä¾èµ–
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,  # âœ… å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œè¡¨ç¤ºä¸¤ä¸ªåˆ†æ”¯
            bias=False
        )

        # nanoç‰ˆ æ›¿æ¢RowParallelLinearä¾èµ–
        self.down_proj = RowParallelLinear(
            intermediate_size, 
            hidden_size, 
            bias=False
        )

        # æ¿€æ´»å‡½æ•°æ£€æŸ¥ï¼Œä¸ºäº†ä»£ç å¥å£®æ€§ï¼Œå®åˆ™æ²¡å•¥ç”¨ï¼Œå°±æ˜¯ä¸ºäº†é˜²æ­¢ä¼ å…¥å¥‡æ€ªçš„æ¿€æ´»å‡½æ•°ã€‚
        if hidden_act.lower() != "silu":
            raise ValueError(
                f"Unsupported activation: {hidden_act}. Only silu is supported for now."
            )
        self.act_fn = SiluAndMul()
        self.prefix = prefix
        self.debug_id = 0

    def forward(self, x):
        gate_up  = self.gate_up_proj(x) 
        act_x = self.act_fn(gate_up)
        ret_x = self.down_proj(act_x)

        return ret_x

    @torch.no_grad()
    def forward_debug(self, x):
        """é€æ­¥è¿”å›æ‰€æœ‰ä¸­é—´ç»“æœï¼Œæ–¹ä¾¿å’Œ vLLM dump çš„å¼ é‡é€å±‚å¯¹é½ã€‚"""
        # x: [B, hidden_size]
        out = {}
        out["x_in"] = x

        gate_up = self.gate_up_proj(x)           # [B, 2 * intermediate]
        out["gate_up"] = gate_up

        # MergedColumnParallelLinear: ä¸¤ä¸ªåˆ†æ”¯æ‹¼åœ¨æœ€åä¸€ç»´
        gate, up = gate_up.chunk(2, dim=-1)      # å„ [B, intermediate]
        out["gate"] = gate
        out["up"] = up

        # SiluAndMul: silu(gate) * up
        gate_act = torch.nn.functional.silu(gate)
        act_x = gate_act * up                    # [B, intermediate]
        out["act_x"] = act_x

        down = self.down_proj(act_x)             # [B, hidden_size]
        out["down"] = down
        out["ret_x"] = down                      # å’Œ forward è¿”å›ä¿æŒä¸€è‡´

        return out


    def load_weights(self, model_dir: str, prefix: str):
            """ä» HF/GLM-4.5 æƒé‡ä¸­åŠ è½½å½“å‰ MLP å±‚å‚æ•°"""

            weight_files = sorted(glob(os.path.join(model_dir, "*.safetensors")))
            # print(f"ğŸ“¦ æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡åˆ†ç‰‡")

            gate_w = None
            up_w = None
            down_w = None

            for wf in weight_files:
                with safe_open(wf, framework="pt") as f:
                    for name in f.keys():
                        if not name.startswith(prefix):
                            continue

                        tensor = f.get_tensor(name)

                        if name.endswith("gate_proj.weight"):
                            # print(f"âœ… åŠ è½½ {name} ({list(tensor.shape)})")
                            gate_w = tensor  # [intermediate, hidden]

                        elif name.endswith("up_proj.weight"):
                            # print(f"âœ… åŠ è½½ {name} ({list(tensor.shape)})")
                            up_w = tensor    # [intermediate, hidden]

                        elif name.endswith("down_proj.weight"):
                            # print(f"âœ… åŠ è½½ {name} ({list(tensor.shape)})")
                            down_w = tensor  # [hidden, intermediate]

            assert gate_w is not None, "gate_proj.weight æœªæ‰¾åˆ°"
            assert up_w is not None, "up_proj.weight æœªæ‰¾åˆ°"
            assert down_w is not None, "down_proj.weight æœªæ‰¾åˆ°"

            # ğŸ”¥ æ‹¼æ¥ gate + up -> gate_up_proj
            gate_up = torch.cat([gate_w, up_w], dim=0)   # [2*intermediate, hidden]
            # print(f"ğŸ“ æ‹¼æ¥å gate_up å½¢çŠ¶: {list(gate_up.shape)}")
            # print(f"ğŸ“ æ¨¡å— gate_up_proj.weight å½¢çŠ¶: {list(self.gate_up_proj.weight.shape)}")

            assert self.gate_up_proj.weight.shape == gate_up.shape, \
                f"gate_up_proj shape mismatch: module={self.gate_up_proj.weight.shape}, tensor={gate_up.shape}"

            # âœ… ä¸è¦è½¬ç½®ï¼Œå½¢çŠ¶å·²ç»æ˜¯ [out, in]
            self.gate_up_proj.weight.data.copy_(gate_up)

            # down_proj ä¹Ÿç›´æ¥å¤åˆ¶ï¼Œä¸è¦è½¬ç½®
            assert self.down_proj.weight.shape == down_w.shape, \
                f"down_proj shape mismatch: module={self.down_proj.weight.shape}, tensor={down_w.shape}"
            self.down_proj.weight.data.copy_(down_w)

            # print("ğŸ¯ MLP æƒé‡åŠ è½½å®Œæˆï¼")
    # def load_weights(self, state_dict: dict, prefix: str):
    #     """ä»state_dictä¸­åŠ è½½MLPçš„æƒé‡ï¼ˆgate_projã€up_projã€down_projï¼‰"""
    #     gate_w = None
    #     up_w = None
    #     down_w = None

    #     for name, tensor in state_dict.items():
    #         if not name.startswith(prefix):
    #             continue

    #         if name.endswith("gate_proj.weight"):
    #             gate_w = tensor
    #         elif name.endswith("up_proj.weight"):
    #             up_w = tensor
    #         elif name.endswith("down_proj.weight"):
    #             down_w = tensor

    #     assert gate_w is not None, f"gate_proj.weight not found in prefix {prefix}"
    #     assert up_w is not None, f"up_proj.weight not found in prefix {prefix}"
    #     assert down_w is not None, f"down_proj.weight not found in prefix {prefix}"

    #     # æ‹¼æ¥gateå’Œupçš„æƒé‡åˆ°gate_up_proj
    #     gate_up = torch.cat([gate_w, up_w], dim=0).to(self.gate_up_proj.weight.dtype)
    #     self.gate_up_proj.weight.data.copy_(gate_up)

    #     # åŠ è½½down_projçš„æƒé‡
    #     down_w = down_w.to(self.down_proj.weight.dtype)
    #     self.down_proj.weight.data.copy_(down_w)
